---
title: "Homework 4: Solution"
output: html_document
---

# Exercise 1: Framingham Heart Study

We consider the following: 

$$\small \Pr(\color{#e64173}{\text{CHD}_i} = 1| \color{#6A5ACD}{\text{BMI}_i}, \color{#20B2AA}{\text{SEX}_i}, \color{#FFA500}{\text{AGE}_i}, \color{#8bb174}{\text{SYSBP}_i},  \color{#314f4f}{\text{DIABP}_i}) = g\left(\beta_0 + \beta_1\color{#6A5ACD}{\text{BMI}_i} + \beta_2 \color{#20B2AA}{\text{SEX}_i} + \beta_3 \color{#FFA500}{\text{AGE}_i} + \beta_4 \color{#8bb174}{\text{SYSBP}_i} + \beta_5 \color{#314f4f}{\text{DIABP}_i}\right),$$
where we define $g(x) = \exp(x)/(1 + \exp(x))$.

We start by estimating the parameters of this model:

```{r, warning=F}
library(riskCommunicator)
data(framingham)

mod = glm(PREVCHD ~ BMI + SEX + AGE + SYSBP + DIABP, data = framingham, family = binomial())
summary(mod)
```

Then, we evaluate the validity of the different claims:

**Claim 1**: "*Men are more likely than women to be affected by a coronary heart disease*" which corresponds to the hypotheses:

$$H_0: \;\; \beta_2 = 0, \;\;\; H_a: \;\; \beta_2 < 0.$$
The p-value of this test is nearly 0. Therefore, we can conclude at the 95% confidence level that men are more likely than women to be affected by a coronary heart disease.

**Claim 2**: "*Individuals with a higher systolic blood pressure are more likely to be affected by a coronary heart disease*" which corresponds to the hypotheses:

$$H_0: \;\; \beta_4 = 0, \;\;\; H_a: \;\; \beta_4 > 0.$$

The p-value of this test is nearly 0. Therefore, we can conclude at the 95% confidence level that individuals with a higher systolic blood pressure are more likely to be affected by a coronary heart disease.

**Claim 3**: "*Individuals with a higher body mass index are more likely to be affected by a coronary heart disease*" which corresponds to the hypotheses:

$$H_0: \;\; \beta_1 = 0, \;\;\; H_a: \;\; \beta_1 > 0.$$

The p-value of this test is 0.121%. Therefore, we can conclude at the 95% confidence level that individuals with a higher body mass index are more likely to be affected by a coronary heart disease.

**Claim 4**: "*Individuals with a lower diastolic blood pressure are more likely to be affected by a coronary heart disease*" which corresponds to the hypotheses:

$$H_0: \;\; \beta_5 = 0, \;\;\; H_a: \;\; \beta_5 < 0.$$

The p-value of this test is 4.327%. Therefore, we can conclude at the 95% confidence level that individuals with a lower diastolic blood pressure are more likely to be affected by a coronary heart disease.

Based on our model we can predict the probability that a 50 years old men with a BMI of 25, systolic blood pressure of 120 and diastolic blood pressure of 85 is affected by a coronary heart disease as follows:

```{r}
predict(mod, data.frame(BMI = 25, SEX = 1, AGE = 50, SYSBP = 120, DIABP = 85), type = "response")
```
 
We can compare this probability with a person having a BMI of 50:

```{r}
predict(mod, data.frame(BMI = 50, SEX = 1, AGE = 50, SYSBP = 120, DIABP = 85), type = "response")
```

As expected, a larger BMI leads to a higher probability of being affected by a coronary heart disease.

# Exercise 2: Pima Indians Diabetes

We start by fitting an initial model including all available variables:

```{r}
library(mlbench)
data(PimaIndiansDiabetes)
mod1 = glm(diabetes ~ ., data = PimaIndiansDiabetes, family = binomial())
summary(mod1)
```

We can see that among these 8 variables, some appear not significant, implying that we may be able to find a smaller model with less variables. We evaluate the validity of the different claims based on this first model:

**Claim 1**: "*Women with a higher body mass index are more likely to be affected by diabetes*" which corresponds to a p-value of nearly 0 (what are the hypotheses here? ðŸ¤”). Therefore, we can conclude at the 95% confidence level that women with a higher body mass index are more likely to be affected by diabetes.

**Claim 2**: "*Older women are more likely to be affected by diabetes*" which corresponds to a p-value of nearly 5.56% (what are the hypotheses here? ðŸ¤”). Therefore, we cannot reject the null hypothesis and there isn't enough evidence to conclude at the 95% confidence level that age is determining factor.

Naturally, Model 1 is not the best possible model to describe our data. Actually, no matter how we select our model we will never be able to claim: "this is the best model!". In this case, there are 8 variables corresponding to $2^8 = 256$ possible models so it would be possible to explore all models. Nevertheless, we will consider an alternative approach based on the stepwise forward AIC method which can be performed as follows:

```{r}
# Initial model
smallest = glm(diabetes ~ 1, data = PimaIndiansDiabetes, family = binomial())

# Find a model with a forward approach using the AIC
mod2 = step(smallest, scope = list(lower = formula(smallest), upper = formula(mod1)), direction = "forward", trace = FALSE)
summary(mod2)
```

It can be observed that Model 2 is nearly identical to Model 1 but the variable `triceps` was removed. It can be observed in the summary of Model 1 that this variable indeed does not appear to play an important role. Since the total number of variables is small (i.e. only 8 variables), Models 1 and 2 are quite close and provide very similar p-values (so won't change our conclusions for the claims of interest). However, this is not always the case, in particular when the number of variables is large (in many modern studies we are considering thousands of variables). 

However, we can verify that Model 2 has a smaller AIC than Model 1:

```{r}
AIC(mod1)
AIC(mod2)
```

Since the two models are quite "close" we don't expect to see large differences in terms of in-sample and out-of-sample predictions. First, we consider the in-sample classification accuracy of the models:

```{r}
# Model 1
class_predict = mod1$fitted.values > 0.5
in_accuracy_mod1 = mean((PimaIndiansDiabetes$diabetes == "pos") == class_predict)

# Model 2
class_predict = mod2$fitted.values > 0.5
in_accuracy_mod2 = mean((PimaIndiansDiabetes$diabetes == "pos") == class_predict)

in_accuracy_mod1
in_accuracy_mod2
```

As expected, the in-sample prediction performances are nearly identical. Next, we consider the out-of-sample classification accuracy of the models:

```{r}
library(boot)

cost = function(resp, pred){
  mean(resp == (pred > 0.5))
}

out_accuracy_mod1 = cv.glm(PimaIndiansDiabetes, mod1, cost, K = 10)$delta[2]
out_accuracy_mod2 = cv.glm(PimaIndiansDiabetes, mod2, cost, K = 10)$delta[2]

out_accuracy_mod1
out_accuracy_mod2
```

Once again, the results are very close and the performances are nearly identical.